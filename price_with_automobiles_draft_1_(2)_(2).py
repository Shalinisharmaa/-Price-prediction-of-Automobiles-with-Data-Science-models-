# -*- coding: utf-8 -*-
"""Price_With_Automobiles_Draft_1 (2) (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNL8RoiCEPN-7o92JR0U4zKBNOJG_87_
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objs as go
import plotly.express as px
import warnings
warnings.filterwarnings("ignore")
from scipy import stats
from plotly.subplots import make_subplots
#import sweetviz as sv
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
from scipy.stats import randint

df = pd.read_csv("data.csv")

df.head()

df.shape

df.size

df.info()

df['Price'].notna().sum()

df.describe()

df.isnull().sum()

df.isnull().sum()[df.isnull().sum() > 2]

nan_value = df.isna().sum(axis=1)
df_cleaned = df[nan_value <= 2]
print(f'Number of rows after removing those with more than 2 NaN values: {df_cleaned.shape[0]}')
count_perrow_cleaned = df_cleaned.isna().sum(axis=1)
cleaned_more_2_nans = count_perrow_cleaned[count_perrow_cleaned > 2]
overall = cleaned_more_2_nans.count()
print(f'Number of rows with more than 2 NaN values after cleaning: {overall}')

print(df_cleaned.isnull().sum())

columns_to_remove = ['VIN', 'InteriorColor']
df_cleaned = df_cleaned.drop(columns=columns_to_remove)
print(df_cleaned.columns)

df_cleaned.shape

df_cleaned.isnull().sum()

# Group by 'Category' and count NaN values in 'Color'
nan_counts_by_category = df_cleaned.groupby('Category')['Color'].apply(lambda x: x.isna().sum())

# Print the results
print("Number of NaN values in 'Color' by category:")
print(nan_counts_by_category)

##df_cleaned = df_cleaned[df_cleaned['Price'] != 0]

df_cleaned = df_cleaned.dropna(subset=['Color'])

df_cleaned.isnull().sum()

df_cleaned.describe()

df_cleaned['Price'].max()

#column = 'Price'  # Replace 'Price' with the column name you want to plot

 # Assume df_cleaned is your DataFrame and 'column' is the column name you're plotting
#df_cleaned_for_plotting = df_cleaned.reset_index(drop=True)

# # Scatter plot for data
 #sns.scatterplot(x=df_cleaned_for_plotting.index, y=df_cleaned_for_plotting[column], label='Data')

# # Ensure outliers_iqr is a boolean mask or list of indices within the range of df_cleaned_for_plotting
# outliers_iqr = [i for i in outliers_iqr if i < len(df_cleaned_for_plotting)]

# # Highlight outliers
 #plt.scatter(df_cleaned_for_plotting.index[outliers_iqr], df_cleaned_for_plotting.loc[outliers_iqr, column],
             #color='red', label='Outliers', marker='o')

#plt.title(f'Scatter Plot of {column}')
#plt.xlabel('Index')
#plt.ylabel(column)
#plt.legend()
#plt.show()

# Plot the boxplot for 'Price'
#plt.figure(figsize=(10, 6))
#sns.boxplot(x=df_cleaned['Price'], color='lightblue')
#plt.title('Boxplot of Price')
#plt.xlabel('Price')
#plt.show()

df_cleaned = df_cleaned[df_cleaned['Price']< 100000]

df_cleaned['FuelType'] = df_cleaned['FuelType'].fillna('other')
df_cleaned['FuelType'].value_counts(dropna = False)
print("Unique values in 'FuelType' column:", df_cleaned['FuelType'].unique())

print("Unique values in 'DriveWheels' column:", df_cleaned['DriveWheels'].unique())

print("Unique values in 'EngineVolume' column:", df_cleaned['EngineVolume'].unique())

#df_cleaned['Doors'].fillna('2', inplace=True)
df_cleaned['Doors'].fillna(method='bfill', inplace=True)
df_cleaned['Doors'].value_counts(dropna = False)
print("Unique values in 'Doors' column:", df_cleaned['Doors'].unique())

df_cleaned['GearBox'].value_counts(dropna=False)

df_cleaned['GearBox'] = df_cleaned['GearBox'].fillna('other')

# Set the plot style
sns.set_style('whitegrid')

# Create the box plot
plt.figure(figsize=(12, 6))
sns.boxplot(x='GearBox', y='Price', data=df_cleaned, palette='Set2')

# Set plot labels and title
plt.title('Price Distribution by GearBox Type', fontsize=16)
plt.xlabel('GearBox Type', fontsize=14)
plt.ylabel('Price', fontsize=14)

# Adjust x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.tight_layout()
plt.show()

df_cleaned['GearBox'] = df_cleaned['GearBox'].fillna('other')

# Calculate the percentage distribution of each GearBox type
gearbox_counts = df_cleaned['GearBox'].value_counts(normalize=True) * 100

# Define the labels and their corresponding sizes
labels = gearbox_counts.index
sizes = gearbox_counts.values

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99','#c2c2f0'],
        wedgeprops={'edgecolor': 'black'}, textprops={'fontsize': 12})

# Equal aspect ratio ensures that pie is drawn as a circle.
plt.axis('equal')

# Add a title
plt.title('GearBox Distribution with Percentages', fontsize=16)

# Show the plot
plt.tight_layout()
plt.show()

df_cleaned['DriveWheels'].fillna(df_cleaned['DriveWheels'].mode()[0], inplace=True)
print(f'Number of null values in "DriveWheels" after filling: {df_cleaned["DriveWheels"].isna().sum()}')

count_drive_wheel = df_cleaned['DriveWheels'].value_counts(dropna=False)
print(f'Value counts for "DriveWheels":\n{count_drive_wheel}')
# Plot distribution of 'Price' based on 'DriveWheels'
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_cleaned, x='DriveWheels', y='Price', palette="Set2", showfliers=False)
plt.title('Price Distribution by DriveWheels')
plt.xlabel('DriveWheels')
plt.ylabel('Price')
plt.xticks(rotation=45)
plt.show()

df_cleaned.hist(column='Price', bins=10, grid=False, figsize=(10, 5), color='salmon', edgecolor='black')

plt.hist(df_cleaned['Price'])

plt.hist(np.cbrt(df_cleaned['Price']))

#plt.hist(np.log(df_cleaned['Price']))

df_cleaned['Price'] =  np.cbrt(df_cleaned['Price'])

plt.hist(df_cleaned['Price'])

df_cleaned['EngineVolume'] = pd.to_numeric(df_cleaned['EngineVolume'], errors='coerce')
# Calculate the median of the 'EngineVolume' column, excluding NaN values
median_engine_volume = df_cleaned['EngineVolume'].median()
# Fill missing values (NaNs) in 'EngineVolume' with the median value
df_cleaned['EngineVolume'].fillna(median_engine_volume, inplace=True)

print("Unique values in 'EngineVolume' column:", df_cleaned['EngineVolume'].unique())

print("Unique values in 'Doors' column:", df_cleaned['Doors'].unique())

fuel_price = df_cleaned.groupby('FuelType')['Price'].mean().reset_index()

# Sort the data for better visualization
fuel_price = fuel_price.sort_values(by='Price', ascending=False)

# Set the plot style
sns.set_style('whitegrid')

# Create the bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='FuelType', y='Price', data=fuel_price, palette='viridis')

# Set plot labels and title
plt.title('Average Price by Fuel Type', fontsize=16)
plt.xlabel('Fuel Type', fontsize=14)
plt.ylabel('Average Price', fontsize=14)

# Show the plot
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 8))
sns.boxplot(data=df_cleaned, x='Doors', y='Price', palette="Set2", showfliers=False)  # `showfliers=False` to exclude extreme outliers for clearer visualization
plt.title('Price Distribution by Doors Type')
plt.xticks(rotation=45)  # Rotate x labels if necessary for better readability
plt.xlabel('Doors Type')
plt.ylabel('Price')
plt.tight_layout()
plt.show()

# Fill NaN in 'Model' based on 'Manufacturer'
df_cleaned['Model'] = df_cleaned.groupby('Manufacturer')['Model'].transform(lambda x: x.fillna(x.mode()[0]))

# Define a function to detect outliers using IQR method
def detect_outliers_iqr(df1, columns):
    outlier_indices = []

    for column in columns:
        if df[column].dtype in [np.float64, np.int64]:  # Only apply IQR to numeric columns
            Q1 = df1[column].quantile(0.25)
            Q3 = df1[column].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = df1[(df1[column] < lower_bound) | (df1[column] > upper_bound)].index
            outlier_indices.extend(outliers)

    # Get unique indices to avoid duplicate entries
    return list(set(outlier_indices))

# Columns to check for outliers
numeric_columns = ['Year', 'Mileage', 'EngineVolume', 'Price', 'Clearance']
outliers_iqr = detect_outliers_iqr(df_cleaned, numeric_columns)

# Print number of outliers detected
print(f"Number of outliers detected using IQR: {len(outliers_iqr)}")

# Optional: Drop outliers from df_cleaned
df_clean = df_cleaned.drop(index=outliers_iqr)

# Print number of rows after removing outliers
print(f"Number of rows after removing outliers using IQR: {df_clean.shape[0]}")

# Define columns for scatter plot
scatter_columns = ['Year', 'Mileage', 'EngineVolume', 'Price']  # Example columns for scatter plots

# Prepare the plotting data
df_cleaned_for_plotting = df_cleaned.copy()

# Ensure numeric columns are properly formatted
for column in scatter_columns:
    df_cleaned_for_plotting[column] = pd.to_numeric(df_cleaned_for_plotting[column], errors='coerce')

# Plot scatter plots with outliers
plt.figure(figsize=(15, 10))
for i, column in enumerate(scatter_columns, 1):
    plt.subplot(2, 2, i)

    # Scatter plot of data
    sns.scatterplot(x=df_cleaned_for_plotting.index, y=df_cleaned_for_plotting[column], label='Data')

    # Highlight outliers
    plt.scatter(df_cleaned.loc[outliers_iqr].index, df_cleaned.loc[outliers_iqr, column], color='red', label='Outliers', marker='o')

    plt.title(f'Scatter Plot of {column}')
    plt.xlabel('Index')
    plt.ylabel(column)
    plt.legend()

plt.tight_layout()
plt.show()

# Function to detect outliers using the IQR method
def detect_outliers_iqr(df, columns):
    outliers = pd.Series([False] * len(df), index=df.index)
    for column in columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers |= (df[column] < lower_bound) | (df[column] > upper_bound)
    return outliers

# Recompute outliers for the columns of interest
outliers_iqr = detect_outliers_iqr(df_cleaned_for_plotting, scatter_columns)

# Plot scatter plots with outliers
plt.figure(figsize=(15, 10))
for i, column in enumerate(scatter_columns, 1):
    plt.subplot(2, 2, i)

    # Scatter plot of data
    sns.scatterplot(x=df_cleaned_for_plotting.index, y=df_cleaned_for_plotting[column], label='Data')

    # Highlight outliers
    plt.scatter(df_cleaned_for_plotting.loc[outliers_iqr].index,
                df_cleaned_for_plotting.loc[outliers_iqr, column],
                color='red', label='Outliers', marker='o')

    plt.title(f'Scatter Plot of {column}')
    plt.xlabel('Index')
    plt.ylabel(column)
    plt.legend()

plt.tight_layout()
plt.show()

# Set Seaborn style
sns.set_style("whitegrid")

# Customize Matplotlib settings
plt.rc(
    "figure",
    autolayout=True,
    figsize=(14, 7),  # Increase figure size
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=16,
    titlepad=10,
)



# Aggregating data by year and plotting average price per year
df_agg = df_cleaned.groupby('Year')['Price'].mean().reset_index()

fig, ax = plt.subplots()
sns.lineplot(x='Year', y='Price', data=df_clean, marker='o', ax=ax)
ax.set_title('Average Car Sales Price by Year')
ax.set_xlabel('Year')
ax.set_ylabel('Average Price')
plt.show()

df_cleaned.hist(column='Price', bins=10, grid=False, figsize=(10, 5), color='salmon', edgecolor='black')

plt.hist(df_cleaned['Price'])

plt.hist(np.cbrt(df_cleaned['Price']))

#plt.hist(np.log(df_cleaned['Price']))

df_cleaned['Price'] =  np.cbrt(df_cleaned['Price'])

#plt.hist(df_cleaned['Price'])

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Assuming df_clean is your cleaned DataFrame

# Separate features and target variable
X = df_cleaned.drop(columns=['Price'])
y = df_cleaned['Price']

# Identify categorical and numerical columns
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Create preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing pipelines
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ])

# Create a preprocessing and modeling pipeline
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LinearRegression())
])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model_pipeline.fit(X_train, y_train)

# Make predictions
y_pred = model_pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

# prompt: do feature engineering for best feature for prediction with columns weight in % Asc

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_regression

# Apply SelectKBest with f_regression to get feature importances
selector = SelectKBest(score_func=f_regression, k='all')
selector.fit(model_pipeline.named_steps['preprocessor'].transform(X_train), y_train)

# Get feature scores and column names
feature_scores = selector.scores_
feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()

# Create a DataFrame to store feature importances
feature_importances = pd.DataFrame({
    'Feature': feature_names,
    'Score': feature_scores
})

# Sort by importance and calculate percentage weight
feature_importances = feature_importances.sort_values('Score', ascending=True)
feature_importances['Weight (%)'] = (feature_importances['Score'] / feature_importances['Score'].sum()) * 100

print(feature_importances)

# prompt: create graph with Actual price with predicted price


# Create a DataFrame for comparison
comparison_df = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': y_pred})

# Plot the comparison
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Actual Price', y='Predicted Price', data=comparison_df)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices')

# Add a diagonal line for reference
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')

plt.show()

# prompt: create funtion to adjust model intercept with graph with Actual price with predicted price
def adjust_intercept_and_plot(model_pipeline, X_test, y_test, adjusted_intercept):
  """
  Adjusts the intercept of a linear model and plots actual vs predicted prices.

  Args:
    model_pipeline: A fitted scikit-learn Pipeline with a LinearRegression model.
    X_test: The test features.
    y_test: The test target values.
  """

  # Get the original intercept
  original_intercept = model_pipeline.named_steps['model'].intercept_

  # Calculate the mean difference between actual and predicted values
  y_pred = model_pipeline.predict(X_test)
  mean_difference = np.mean(y_test - y_pred)

  # Adjust the intercept

  model_pipeline.named_steps['model'].intercept_ = adjusted_intercept

  # Make predictions with the adjusted intercept
  y_pred_adjusted = model_pipeline.predict(X_test)

  # Create a DataFrame for comparison
  comparison_df = pd.DataFrame({
      'Actual Price': y_test,
      'Predicted Price (Original)': y_pred,
      'Predicted Price (Adjusted)': y_pred_adjusted
  })

  # Plot the comparison
  plt.figure(figsize=(10, 6))
  sns.scatterplot(x='Actual Price', y='Predicted Price (Original)', data=comparison_df, label='Original', alpha=0.5)
  sns.scatterplot(x='Actual Price', y='Predicted Price (Adjusted)', data=comparison_df, label='Adjusted', alpha=0.5)
  plt.xlabel('Actual Price')
  plt.ylabel('Predicted Price')
  plt.title('Actual vs Predicted Prices (Before and After Intercept Adjustment)')

  # Add a diagonal line for reference
  plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')

  plt.legend()
  plt.show()

model_pipeline.named_steps['model'].intercept_

# Call the function to adjust the intercept and plot the results
adjust_intercept_and_plot(model_pipeline, X_test, y_test, 1.67)

# prompt: Provide Q-Q plot for mode

import matplotlib.pyplot as plt
import statsmodels.api as sm

# Fit the model to the training data (assuming model_pipeline is already defined)
model_pipeline.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model_pipeline.predict(X_test)

# Calculate residuals
residuals = y_test - y_pred

# Create a Q-Q plot of the residuals
sm.qqplot(residuals, line='s')
plt.title('Q-Q Plot of Residuals')
plt.show()

# prompt: model_pipeline mean sqare and r squre

# Assuming 'model_pipeline', 'X_test', and 'y_test' are already defined

# Make predictions on the test set
y_pred = model_pipeline.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate R-squared (R^2) score
r2 = r2_score(y_test, y_pred)

# Print the results
print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
# Create polynomial features
poly = PolynomialFeatures(degree=2)  # Adjust the degree as needed


# Separate features and target variable
X = df_cleaned.drop(columns=['Price'])
y = df_cleaned['Price']

# Identify categorical and numerical columns
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Create preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing pipelines
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ])

# Create a new pipeline with polynomial features
poly_model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2)),  # Add polynomial features step
    ('model', LinearRegression())
])

# Fit the polynomial regression model
poly_model_pipeline.fit(X_train, y_train) # The fit method should still use the original X_train

# Make predictions on the test data
y_pred_poly = poly_model_pipeline.predict(X_test)

# Evaluate the polynomial regression model
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print(f"Polynomial Regression - Mean Squared Error: {mse_poly:.2f}")
print(f"Polynomial Regression - R^2 Score: {r2_poly:.2f}")

# prompt: q-q plot for poly_model_pipeline

import matplotlib.pyplot as plt
# Make predictions on the test set using the polynomial model
y_pred_poly = poly_model_pipeline.predict(X_test)

# Calculate residuals for the polynomial model
residuals_poly = y_test - y_pred_poly

# Create a Q-Q plot of the residuals for the polynomial model
sm.qqplot(residuals_poly, line='s')
plt.title('Q-Q Plot of Residuals (Polynomial Regression)')
plt.show()

plot_data = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred
})

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Actual', y='Predicted', data=plot_data, alpha=0.6)
plt.plot([plot_data['Actual'].min(), plot_data['Actual'].max()],
         [plot_data['Actual'].min(), plot_data['Actual'].max()],
         color='red', linestyle='--', linewidth=2)
plt.title('Scatter Plot of Predicted vs Actual Values')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import FunctionTransformer

# Log transformation function
def log_transform(X):
    X = X.copy()
    if 'Mileage' in X.columns:
        X['LogMileage'] = np.log1p(X['Mileage'])  # log1p handles log(0) cases
    if 'EngineVolume' in X.columns:
        X['LogEngineVolume'] = np.log1p(X['EngineVolume'])
    return X[['LogMileage', 'LogEngineVolume']]

# Assuming df_clean is your cleaned DataFrame
X = df_cleaned.drop(columns=['Price'])
y = df_cleaned['Price']

# Identify categorical and numerical columns
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Create preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing pipelines with a feature engineering step
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features),
        ('log', FunctionTransformer(log_transform), ['Mileage', 'EngineVolume'])
    ])

# Create a preprocessing and modeling pipeline
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', LinearRegression())
])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model_pipeline.fit(X_train, y_train)

# Make predictions
y_pred = model_pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")

"""GBR Model"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from skopt import BayesSearchCV
from skopt.space import Real, Integer

# Define features and target
X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

# Define categorical and numerical features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median
    ('scaler', StandardScaler())  # Standardize numerical features
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ]
)

# Define the Gradient Boosting Regressor
gbr = GradientBoostingRegressor()

# Define the parameter search space for Bayesian Optimization with reduced ranges
search_space = {
    'model__n_estimators': Integer(50, 150),  # Narrowed range
    'model__learning_rate': Real(0.05, 0.15, prior='log-uniform'),  # Narrowed range
    'model__max_depth': Integer(3, 8)  # Narrowed range
}

# Create pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', gbr)])

# Bayesian search for hyperparameter tuning
bayes_search = BayesSearchCV(
    pipeline,
    search_space,
    n_iter=10,  # Reduced number of iterations
    cv=3,  # Reduced number of folds
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

# Fit the model with Bayesian Optimization
bayes_search.fit(X, y)

# Best model
best_model = bayes_search.best_estimator_
print(f"Best Parameters: {bayes_search.best_params_}")

# Cross-validation
cv_scores = cross_val_score(best_model, X, y, cv=3, scoring='r2', n_jobs=-1)  # Reduced folds
print(f"Cross-validated R^2 Scores: {cv_scores}")
print(f"Average Cross-validated R^2 Score: {np.mean(cv_scores)}")

# Train-test split and evaluation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")
print("-" * 50)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from scipy.stats import uniform, randint

# Define features and target
X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

# Define categorical and numerical features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median
    ('log', FunctionTransformer(np.log1p, validate=True)),  # Apply log transformation to reduce skewness
    ('scaler', StandardScaler()),  # Standardize numerical features
    ('poly', PolynomialFeatures(degree=2, include_bias=False))  # Generate polynomial features
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ]
)

# Define the Gradient Boosting Regressor and hyperparameters
gbr = GradientBoostingRegressor()

param_distributions = {
    'model__n_estimators': randint(50, 200),
    'model__learning_rate': uniform(0.01, 0.2),
    'model__max_depth': randint(3, 10)
}

# Create pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', gbr)])

# Randomized search for hyperparameter tuning
random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, scoring='r2', n_jobs=-1, random_state=42)
random_search.fit(X, y)

# Best model
best_model = random_search.best_estimator_
print(f"Best Parameters: {random_search.best_params_}")

# Cross-validation
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2')
print(f"Cross-validated R^2 Scores: {cv_scores}")
print(f"Average Cross-validated R^2 Score: {np.mean(cv_scores)}")

# Train-test split and evaluation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")
print("-" * 50)

"""KNN model"""

# Sample DataFrame (replace with your actual DataFrame)
# df_clean = pd.read_csv('your_data.csv')  # Load your data here

# Define features and target
X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

# Define categorical and numerical features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values with median
    ('scaler', StandardScaler()),  # Standardize numerical features
    ('poly', PolynomialFeatures(degree=2, include_bias=False))  # Generate polynomial features
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical features
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ]
)

# Define the K-Nearest Neighbors Regressor and hyperparameters
knn = KNeighborsRegressor()

param_distributions = {
    'model__n_neighbors': randint(3, 20),
    'model__weights': ['uniform', 'distance'],
    'model__p': [1, 2]  # p=1 for Manhattan distance, p=2 for Euclidean distance
}

# Create pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('model', knn)])

# Randomized search for hyperparameter tuning
# Limiting n_jobs to 1 to avoid parallel processing issues
random_search = RandomizedSearchCV(
    pipeline,
    param_distributions,
    n_iter=10,
    cv=5,
    scoring='r2',
    n_jobs=1,  # Change to 1 to avoid parallel execution issues
    random_state=42
)

# Fit the model
random_search.fit(X, y)

# Best model
best_model = random_search.best_estimator_
print(f"Best Parameters: {random_search.best_params_}")

# Cross-validation
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=1)  # Avoid parallel execution here as well
print(f"Cross-validated R^2 Scores: {cv_scores}")
print(f"Average Cross-validated R^2 Score: {np.mean(cv_scores)}")

# Train-test split and evaluation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")
print("-" * 50)

# Define features and target
X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

# Define categorical and numerical features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('pca', PCA(n_components=0.95))
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ]
)

# Define the K-Nearest Neighbors Regressor
knn = KNeighborsRegressor()

# Reduced hyperparameter grid
param_grid = {
    'model__n_neighbors': [5, 10, 15],  # Reduced range
    'model__weights': ['uniform', 'distance'],
    'model__p': [1, 2]  # Manhattan and Euclidean only
}

# Create pipeline with preprocessing and model
pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                           ('feature_selection', SelectKBest(f_regression, k='all')),
                           ('model', knn)])

# Grid search for hyperparameter tuning with limited parallelism
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=1,  # Avoid parallel processing to save memory
    verbose=2
)

grid_search.fit(X, y)

# Best model
best_model = grid_search.best_estimator_
print(f"Best Parameters: {grid_search.best_params_}")

# Cross-validation
cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=1)
print(f"Cross-validated R^2 Scores: {cv_scores}")
print(f"Average Cross-validated R^2 Score: {np.mean(cv_scores)}")

# Train-test split and evaluation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

"""Commen code"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Handling missing values
    ('scaler', StandardScaler())  # Scaling numerical features
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handling missing values
    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encoding categorical features
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_features),
        ('cat', categorical_pipeline, categorical_features)
    ])

#Define a function to run different models
def run_model(model, model_name):
    # Create a pipeline with the preprocessor and the model
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{model_name} - Mean Squared Error: {mse:.2f}")
    print(f"{model_name} - R^2 Score: {r2:.2f}\n")

run_model(RandomForestRegressor(n_estimators=100, random_state=42), 'Random Forest Regressor')

run_model(DecisionTreeRegressor(random_state=42), 'Decision Tree Regressor')

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
import seaborn as sns

X = df_clean.drop(columns=['Price'])
y = df_clean['Price']

# Define categorical and numerical features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Preprocessing pipeline
numerical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),  # Handle missing values for numerical features
    ('scaler', StandardScaler())  # Scale numerical features
])

categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values for categorical features
    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical features
])

# Combine numerical and categorical pipelines
preprocessor = ColumnTransformer([
    ('num', numerical_pipeline, numerical_features),
    ('cat', categorical_pipeline, categorical_features)
])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a function to run and evaluate a model
def run_model(model, model_name):
    # Create a pipeline that combines preprocessing with the model
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    # Fit the model
    pipeline.fit(X_train, y_train)

    # Make predictions
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"{model_name} - Mean Squared Error: {mse:.2f}")
    print(f"{model_name} - R^2 Score: {r2:.2f}")

    # Plot Actual vs Predicted
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('Actual Price')
    plt.ylabel('Predicted Price')
    plt.title(f'Actual vs Predicted Prices for {model_name}')
    plt.show()

# Run different models
# 1. RandomForest Regressor
run_model(RandomForestRegressor(n_estimators=50, random_state=42), 'RandomForestRegressor')

# 2. DecisionTree Regressor
run_model(DecisionTreeRegressor(random_state=42), 'DecisionTreeRegressor')

# 3. Hyperparameter tuning with GridSearchCV for Random Forest
param_grid = {
    'model__n_estimators': [50, 100, 200],
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10],
}

# Combine preprocessing and model in a pipeline
pipeline_rf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(random_state=42))
])

# Grid Search for best hyperparameters
grid_search = GridSearchCV(pipeline_rf, param_grid, cv=5, scoring='r2', verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validation R^2: {grid_search.best_score_:.2f}")

# Evaluate the best model from GridSearchCV
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test)
mse_best = mean_squared_error(y_test, y_pred_best)
r2_best = r2_score(y_test, y_pred_best)

print(f"Best Model - Mean Squared Error: {mse_best:.2f}")
print(f"Best Model - R^2 Score: {r2_best:.2f}")

# Plot Actual vs Predicted for the best model
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_best, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices for Best Model')
plt.show()

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

pipeline.fit(X_train, y_train)

# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Improved Random Forest Regressor - Mean Squared Error: {mse:.2f}")
print(f"Improved Random Forest Regressor - R^2 Score: {r2:.2f}")

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Define your features
categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType',
                        'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Create a pipeline with the preprocessor and RandomForestRegressor
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestRegressor(random_state=42))
])

# Define the hyperparameter grid
param_grid = {
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [10, 20, None],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4],
    'model__max_features': ['auto', 'sqrt']
}

# Set up GridSearchCV with the pipeline
grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

# Extract the best model from GridSearchCV
best_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Best Parameters: {grid_search.best_params_}")
print(f"Random Forest Regressor - Mean Squared Error: {mse}")
print(f"Random Forest Regressor - R^2 Score: {r2}")

import pandas as pd
from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X[numerical_features])

# Add polynomial features to the original data
X_poly_df = pd.DataFrame(X_poly, columns=poly.get_feature_names_out(numerical_features))
X_combined = pd.concat([X, X_poly_df], axis=1)

import optuna
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Define the ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])



def objective(trial):
    rf = RandomForestRegressor(
        n_estimators=trial.suggest_int('n_estimators', 50, 300),
        max_depth=trial.suggest_int('max_depth', 10, 50),
        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),
        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 5),
        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2']),
        random_state=42
    )

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', rf)
    ])

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

print(f"Best parameters: {study.best_params}")
print(f"Best MSE: {study.best_value}")

import matplotlib.pyplot as plt

# Extract data for plotting
trials = study.trials
n_estimators = [trial.params['n_estimators'] for trial in trials]
mse_values = [trial.value for trial in trials]

# Plot MSE versus n_estimators
plt.figure(figsize=(10, 6))
plt.scatter(n_estimators, mse_values, alpha=0.7, edgecolors='w', s=100)
plt.xlabel('Number of Estimators')
plt.ylabel('Mean Squared Error')
plt.title('MSE vs. Number of Estimators')
plt.grid(True)
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Define the parameter grid for RandomizedSearchCV
param_distributions = {
    'model__n_estimators': [100, 200, 300, 400],
    'model__max_depth': [10, 20, 30, None],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4],
    'model__max_features': ['auto', 'sqrt', 'log2']  # Corrected this line
}

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_distributions,
    n_iter=50,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Extract the best model from RandomizedSearchCV
best_model = random_search.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Best Parameters: {random_search.best_params_}")
print(f"Random Forest Regressor - Mean Squared Error: {mse}")
print(f"Random Forest Regressor - R^2 Score: {r2}")

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Define your categorical and numerical features
#categorical_features = ['Manufacturer', 'Model', 'Category', 'FuelType', 'DriveWheels', 'GearBox', 'Doors', 'Wheel', 'LeatherInterior']
#numerical_features = ['Year', 'Mileage', 'EngineVolume', 'Clearance']

# Create preprocessing pipelines for both numerical and categorical data
numerical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine both pipelines into a single preprocessor
preprocessor = ColumnTransformer([
    ('num', numerical_pipeline, numerical_features),
    ('cat', categorical_pipeline, categorical_features)
])

# Preprocess the training and test data
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Get the best parameters from the study
best_params = study.best_params

# Initialize and train the model with the best parameters
best_model = RandomForestRegressor(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    min_samples_leaf=best_params['min_samples_leaf'],
    max_features=best_params['max_features']
)

# Fit the model on preprocessed data
best_model.fit(X_train_preprocessed, y_train)

# Make predictions
y_pred = best_model.predict(X_test_preprocessed)

# Evaluate performance
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error with best parameters: {mse}")

from optuna.visualization import plot_optimization_history

plotly_config = {"staticPlot": True}

fig = plot_optimization_history(study)
fig.show(config=plotly_config)

from optuna.visualization import plot_param_importances

fig = plot_param_importances(study)
fig.show(config=plotly_config)

