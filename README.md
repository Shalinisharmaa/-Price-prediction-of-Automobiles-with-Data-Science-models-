# -Price-prediction-of-Automobiles-with-Data-Science-models-
Price prediction of Automobiles with Data Science models  This project will develop and evaluate a number of machine learning models for predicting used automobile prices. The research will make use of the comprehensive dataset from Kaggle, including a number of vehicle attributes like mileage, fuel type, engine specifications, and market trends. Key preprocessing steps included handling missing values, outlier removal, feature engineering, and data normalization. Various predictive models were applied in this study, comparing their performances: Random Forest, Gradient Boosting, K-Nearest Neighbors, Decision Tree, and Linear Regression. The best performance amongst those is from the Random Forest model, with an R2 score of 0.63 and Mean Squared Error of 29.81. This project provides actionable insights for stakeholders in the automotive industry and also showcases the effective usage of data science in real-world applications. Library, technology and tools used: - Python, Pandas, Scikit-learn, Matplotlib, Seaborn, Plotly, and statistical libraries, Data preprocessing, statistical analysis, visualization,

<img width="426" alt="Screenshot 2025-02-13 at 12 42 23 AM" src="https://github.com/user-attachments/assets/9f9fbf0d-bfdd-46f3-909e-5a0931260cd2" />
Price analysis
The price does not contain any null value in its column but it contains 0 values which can be said that the few vehicles are of no use or trash as it has a price value of 0 or any other reason that has not been confirmed yet. However, some of the analysis has been performed on the price here, so if the dataset is large, this bar graph represents the distribution of the cube roottransformed prices from a dataset as transformation using the cube root (np.cbrt) is commonly applied to reduce the skewness in highly skewed data, mostly in cases where there are extreme values. the first bar, which corresponds to the smallest price range (close to 0 after cube root transformation), has an exceptionally high frequency, nearing 40,000. This suggests that a large number of prices are either very low or clustered near a minimum value, which after transformation, appear in this first bar. Subsequently, another bar, representing slightly higher cube root price ranges (around 5, 10, 15, etc.), show progressively lower counts, generally between 5,000 and 10,000. This indicates that as the prices increase, even after transformation, fewer data points are falling into these higher ranges. This trend continues with frequencies dropping off further beyond the range of 25, where the counts diminish significantly, and the last visible bar (above 30) shows a minimal count close to zero. Despite the transformation, the distribution remains skewed to the right, meaning that most of the prices are still relatively low or closer to a baseline, with only a few prices extending into higher ranges. The transformation helps in spreading out the data, but the overall skewness indicates that the majority of prices are on the lower end. The skewness of the transformed data implies that in the original dataset, a majority of the prices are low, possibly representing a large quantity of vehicles that are more used and now can’t be used. The decline in frequency as prices increase suggests that high-priced vehicles are less common.
